# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1COqBlbOj1Xc-HGlsZeshN-6puy1Abl2S
"""

# prompt: read the titanic dataset and print it out

import pandas as pd
import numpy as np
df = pd.read_csv('/content/sample_data/Titanic.csv')
df.head()

# prompt: You are tasked with using the k-NN algorithm to predict whether a passenger survived the
# Titanic disaster based on features like age, fare, and class.
# 1.1 Explain the k-NN algorithm and how it can be used to classify passengers.

# 1.1 Explain the k-NN algorithm and how it can be used to classify passengers.

# The k-Nearest Neighbors (k-NN) algorithm is a simple, supervised machine learning algorithm used for both classification and regression tasks.
# In the context of the Titanic dataset, we can use it for classification, predicting whether a passenger survived (1) or not (0).

# Here's how it works:

# 1. Data Preparation: We prepare our dataset by selecting relevant features (e.g., age, fare, class)
#    and the target variable (survival). We then split the data into training and testing sets.

# 2. Distance Calculation: When we want to predict the survival of a new passenger, we calculate the distance
#    between this passenger's feature values and all the passengers in the training dataset.
#    Common distance metrics include Euclidean distance or Manhattan distance.

# 3. Finding Nearest Neighbors: We identify the 'k' nearest neighbors to the new passenger based on the calculated distances.
#    'k' is a user-defined parameter.

# 4. Majority Voting: For classification, we determine the majority class among the 'k' nearest neighbors.
#    For example, if the majority of the 'k' nearest neighbors survived, we predict that the new passenger also survived.

# In the case of the Titanic dataset, we can use k-NN to classify passengers based on their features (age, fare, class, etc.).
# The algorithm identifies the passengers in the training set most similar to a new passenger and uses their survival status to predict
# the new passenger's survival.

# prompt: print the dataset to show the nearest neighbors without comments

import pandas as pd
import numpy as np

df = pd.read_csv('/content/sample_data/Titanic.csv')
print(df)

# prompt: print the dataset to show the nearest neighbors

from sklearn.neighbors import NearestNeighbors

# Assuming you want to find the nearest neighbors based on 'Age' and 'Fare'
X = df[['Age', 'Fare']].fillna(0)  # Fill NaN values with 0 for simplicity

# Create a NearestNeighbors object with k=5 (find 5 nearest neighbors)
knn = NearestNeighbors(n_neighbors=5)

# Fit the model to your data
knn.fit(X)

# Choose a passenger (e.g., the first one) for whom you want to find the nearest neighbors
passenger_index = 0
new_passenger = X.iloc[passenger_index].values.reshape(1, -1)

# Find the k-nearest neighbors
distances, indices = knn.kneighbors(new_passenger)

# Print the nearest neighbors
print("Nearest neighbors for passenger", passenger_index)
for i in range(len(indices[0])):
    neighbor_index = indices[0][i]
    neighbor_distance = distances[0][i]
    print(f"Neighbor {i + 1}: Passenger index {neighbor_index}, Distance: {neighbor_distance}")

# You can then access the original dataframe using these indices to see the details of the nearest neighbors.
# For example: df.iloc[neighbor_index]

# prompt: 4 If some trees in a random forest are less accurate than others, how can you ensure that
# majority voting remains fair? explain the output

# Assuming you want to find the nearest neighbors based on 'Age' and 'Fare'
X = df[['Age', 'Fare']].fillna(0)  # Fill NaN values with 0 for simplicity

# Create a NearestNeighbors object with k=5 (find 5 nearest neighbors)
knn = NearestNeighbors(n_neighbors=5)

# Fit the model to your data
knn.fit(X)

# Choose a passenger (e.g., the first one) for whom you want to find the nearest neighbors
passenger_index = 0
new_passenger = X.iloc[passenger_index].values.reshape(1, -1)

# Find the k-nearest neighbors
distances, indices = knn.kneighbors(new_passenger)

# Print the nearest neighbors
print("Nearest neighbors for passenger", passenger_index)
for i in range(len(indices[0])):
    neighbor_index = indices[0][i]
    neighbor_distance = distances[0][i]
    print(f"Neighbor {i + 1}: Passenger index {neighbor_index}, Distance: {neighbor_distance}")

# You can then access the original dataframe using these indices to see the details of the nearest neighbors.
# For example: df.iloc[neighbor_index]

# prompt:  6.2 Perform k-means clustering on the Titanic dataset. Determine the number of clusters and
# describe them. (5

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Select relevant features for clustering
features = ['Pclass', 'Age', 'Fare']
X = df[features].fillna(0)  # Fill NaN values with 0

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine the optimal number of clusters using the Elbow method
inertia = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

# Based on the elbow method, choose the optimal number of clusters (e.g., 3)
n_clusters = 3  # You can change this based on the elbow plot

# Perform k-means clustering
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
kmeans.fit(X_scaled)

# Add cluster labels to the DataFrame
df['Cluster'] = kmeans.labels_

# Describe the clusters
for i in range(n_clusters):
    print(f"\nCluster {i}:")
    print(df[df['Cluster'] == i][features].describe())

# You can further analyze the clusters by looking at the distribution of other features
# within each cluster, such as survival rate, sex, or embarkation port.

# prompt: 6.3 Visualize the centroids of the clusters and explain what patterns you observe. (

# Get the centroids of the clusters
centroids = kmeans.cluster_centers_

# Convert scaled centroids back to original scale
centroids_original = scaler.inverse_transform(centroids)

# Visualize the centroids
plt.scatter(X['Age'], X['Fare'], c=df['Cluster'])
plt.scatter(centroids_original[:, 1], centroids_original[:, 2], marker='X', s=200, c='red', label='Centroids')
plt.xlabel('Age')
plt.ylabel('Fare')
plt.title('Clusters and Centroids')
plt.legend()
plt.show()

# Explain observed patterns
# Based on the visualization, you can discuss the following:
#   - The location of centroids relative to the data points: Do they appear to be well-positioned
#     in the center of dense areas of data points?
#   - Separation of clusters: How distinct are the clusters visually? Are there any overlapping regions?
#   - Features defining clusters: Do certain features (e.g., age or fare) seem to be more influential in
#     separating the clusters?

# For example, if you observe that the centroids are located in distinct areas of the plot
# and the clusters are well separated, it indicates that the clustering algorithm has effectively
# identified groups of passengers with similar characteristics based on age and fare.
# Conversely, if the clusters overlap significantly, it may suggest that the chosen features
# are not sufficient to clearly differentiate between passenger groups.

# prompt: Compare the strengths and weaknesses of k-NN, Decision Trees, and Neural Networks
# for the Titanic dataset.

# Comparing k-NN, Decision Trees, and Neural Networks for the Titanic Dataset

# Strengths and Weaknesses of Each Algorithm:

# k-Nearest Neighbors (k-NN):

# Strengths:
# - Simple to understand and implement.
# - No training period, making it fast for new data prediction.
# - Can handle non-linear relationships between features and the target.

# Weaknesses:
# - Computationally expensive for large datasets as it requires calculating distances for each new instance.
# - Sensitive to the choice of 'k' and distance metric.
# - Can be affected by irrelevant features and the curse of dimensionality.
# - Doesn't provide insight into the underlying relationships in the data.


# Decision Trees:

# Strengths:
# - Easy to interpret and visualize, providing insights into feature importance.
# - Can handle both categorical and numerical features.
# - Relatively robust to outliers and missing data.
# - Fast for prediction compared to complex models like neural networks.

# Weaknesses:
# - Prone to overfitting, especially with deep trees.
# - Can be unstable, with small changes in data potentially leading to different trees.
# - May not perform well on datasets with complex relationships or high dimensionality.


# Neural Networks:

# Strengths:
# - Powerful for complex datasets with non-linear relationships.
# - Can learn intricate patterns and relationships between features.
# - Can achieve high accuracy when trained appropriately.

# Weaknesses:
# - Requires large amounts of data for training.
# - Computationally expensive and requires significant processing power.
# - Can be difficult to interpret and understand the internal workings of the model.
# - Prone to overfitting if not regularized properly.


# Titanic Dataset Considerations:

# - Data Size: The Titanic dataset is relatively small, which could make neural networks less effective.
# - Feature Importance: Some features like 'Sex', 'Pclass', and 'Age' are likely important for predicting survival.
# - Non-linearity: There could be non-linear relationships between features and survival (e.g., interaction between age and class).

# Algorithm Recommendations:

# - k-NN: Could be a reasonable choice for this dataset as it can handle non-linear relationships and is simple to implement.
#   However, it might not be the most accurate due to potential computational cost and sensitivity to feature scaling.
# - Decision Trees: A good option due to their interpretability, ease of implementation, and ability to handle categorical features.
#   They can provide insights into feature importance and patterns related to survival.
# - Neural Networks: Might be overkill for this dataset due to its size. However, if you have a large amount of data or
#   need to model very complex relationships, they could potentially achieve high accuracy.

# Overall, decision trees and potentially k-NN are good starting points for analyzing the Titanic dataset. Neural networks might be
# considered if you have more data or need to explore more complex relationships.

# It is crucial to experiment with different algorithms, tune their parameters, and evaluate their performance using appropriate
# metrics (e.g., accuracy, precision, recall) to determine the best model for the Titanic dataset.

# prompt: Discuss how each model can be prone to overfitting. What techniques would you use to
# address overfitting for each model?

# ... (Your existing code) ...

# Addressing Overfitting in k-NN

# Overfitting in k-NN can occur when 'k' is too small. With a small 'k', the model becomes sensitive to noise
# and outliers in the training data, leading to poor generalization to new, unseen data.

# Techniques to address overfitting in k-NN:
# 1. Cross-validation: Use techniques like k-fold cross-validation to evaluate the model's performance on different subsets of the data.
#    This helps to identify if the model is overfitting to the training data.
# 2. Feature Selection: Select only the most relevant features for the prediction. This can reduce the dimensionality of the data
#    and improve generalization.
# 3. Increase 'k': Increasing the value of 'k' can help to reduce the impact of noisy data points. By considering more neighbors,
#    the model becomes more robust and less prone to overfitting.
# 4. Data Augmentation: Increasing the size of the dataset can help to improve the generalization performance.
# 5. Distance Weighting: Assign weights to the neighbors based on their distance to the new instance. Closer neighbors
#    can have a larger influence on the prediction, which can help to improve the accuracy of the model.


# Addressing Overfitting in K-Means

# Overfitting in K-Means can occur when the number of clusters (k) is chosen improperly.
# If 'k' is too large, the model can create clusters that are too specific to the training data,
# leading to poor generalization to new data.

# Techniques to address overfitting in K-Means:
# 1. Elbow Method: Use the elbow method to determine the optimal number of clusters. This method involves plotting the within-cluster sum of squares (WCSS)
#    against different values of 'k'. The optimal 'k' is usually where the plot starts to level off, forming an elbow shape.
# 2. Silhouette Score: Use the silhouette score to evaluate the quality of the clusters. This score measures how similar each data point is to its own cluster compared to other clusters.
#    A higher silhouette score indicates better cluster quality and less overfitting.
# 3. Feature Scaling: Standardize or normalize the features before performing K-Means. This ensures that features with larger scales do not dominate the clustering process,
#    which can lead to biased and inaccurate clusters.
# 4. Data Preprocessing: Address missing data or outliers appropriately. These can influence the clustering process and potentially lead to overfitting.


# ... (Rest of your code) ...