# -*- coding: utf-8 -*-
"""exam practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e0XvUGBlAHE7hntHgKCBZkEkuqh_uhjl
"""

# prompt: load the xls dataset

import pandas as pd

# Replace 'your_dataset.xls' with the actual path to your dataset
df = pd.read_excel('pimaDiabetes.xls')

# Print the first few rows of the dataframe to check if it loaded correctly
print(df.head())

summary_stats=df.describe()
print("summary Statistic: \n", summary_stats)

df_filled=df.fillna(df.median(numeric_only=True))
#handling missing values by filling them with the median of the respective columnn
#save the cleaned dataframe to a new csv file
df_filled.to_csv('cleaned_diabetes_data.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#create a bar chart showing the distribution of the data
species_counts=df['Outcome'].value_counts()
plt.title('distribution of the Iris species')
plt.xlabel('species')
plt.ylabel('count')
plt.show()
#calculate the correlation matrix between numerical features
numerical_df=df.select_dtypes(include=['float64','int64'])
correlation_matrix=numerical_df.corr()
print("correlation matrix: \n", correlation_matrix)
#plot a heatmap of the correlation matrix
plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('correlation Matrix Heatmap')

plt.show()

dataset = pd.read_csv('cleaned_diabetes_data.csv')
X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)
print(X_train)

from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred=classifier.predict(X_test)
cm=confusion_matrix(y_test, y_pred)
print(cm)
accuracy=accuracy_score(y_test, y_pred)

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

from matplotlib.colors import ListedColormap
X_set, y_set=sc.inverse_transform(X_train), y_train
X1, X2= np.meshgrid(np.arange(start=X_set[:,0].min()-10, stop=X_set[:,0].max()+10, step=0.25),
                   np.arange(start=X_set[:,1].min()-1000, stop=X_set[:,1].max()+1000, step=0.25))
plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], c=ListedColormap(('red', 'green'))(i), label=j)
    plt.title('Random Forest Classification (Training set)')
    plt.xlabel('Age')
    plt.ylabel('Estimated Salary')
    plt.legend()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

# This line was incorrectly placed and causing the error.
# It should be removed or moved to a more appropriate place in the code
# sc=StandardScaler()
# X_train=sc.fit_transform(X_train)
# X_test=sc.transform(X_test)

from matplotlib.colors import ListedColormap
X_set, y_set=X_train, y_train  # Use X_train instead of inverse_transform
# Creating the meshgrid with all 8 features
X_mesh = np.array([X_set[:, i].ravel() for i in range(X_set.shape[1])])
# Generating a grid of points for prediction
X1, X2 = np.meshgrid(np.arange(start=X_mesh[0].min()-10, stop=X_mesh[0].max()+10, step=0.25),
                   np.arange(start=X_mesh[1].min()-1000, stop=X_mesh[1].max()+1000, step=0.25))

# Reshape the grid to have 8 features like the training data
X_grid = np.array([X1.ravel(), X2.ravel()] + [X_mesh[i].ravel() for i in range(2, X_set.shape[1])]).T

# Now, use sc.transform with X_grid which has 8 features
plt.contourf(X1, X2, classifier.predict(sc.transform(X_grid)).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set==j, 0], X_set[y_set==j, 1], c=ListedColormap(('red', 'green'))(i), label=j)
    plt.title('Random Forest Classification (Training set)')
    plt.xlabel('Age')
    plt.ylabel('Estimated Salary')
    plt.legend()
    plt.show()

"""histogram"""

import plotly.express as px
import pandas as pd
df=pd.read_csv('cleaned_diabetes_data.csv')
fig=px.histogram(df, x='Age', nbins=30, title='Distribution of Age')
fig.show()

import plotly.express as px
import pandas as pd
df=pd.read_csv('cleaned_diabetes_data.csv')
fig=px.histogram(df, x='BMI', nbins=30, title='Distribution of BMI')
fig.show()



from google.colab import drive
drive.mount('/content/drive')



"""Describe the K-Nearest Neighbours (KNN) algorithm. How does KNN classify a new data point,
and what are the key factors that influence its performance?
Answer: The K-Nearest Neighbours (KNN) algorithm is a simple, instance-based learning algorithm
used for classification and regression tasks. KNN classifies a new data point based on the majority
class of its 'k' nearest neighbours in the training dataset. Here's how it works:
o Distance Calculation: The algorithm calculates the distance (commonly Euclidean distance)
between the new data point and all points in the training set.
o Selecting Neighbours: It then identifies the 'k' nearest neighbours (data points) to the new
data point.
o Majority Voting: For classification, the algorithm assigns the most common class among the
'k' neighbours to the new data point. For regression, it takes the average of the 'k' neighbours’
values.
o Key Factors Influencing Performance:
§ Choice of 'k': A small 'k' can make the algorithm sensitive to noise, while a large 'k'
might smooth out local patterns, leading to underfitting.
§ Distance Metric: The choice of distance metric (e.g., Euclidean, Manhattan) affects
how distances are measured and, consequently, the classification.
§ Feature Scaling: Since KNN relies on distance calculations, features should be
normalized or scaled to ensure that no single feature disproportionately influences the
results
"""

